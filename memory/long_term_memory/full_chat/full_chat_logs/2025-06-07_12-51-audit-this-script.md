<!-- Generated by SpecStory -->

# Audit this script. (2025-06-07 07:51:42)

_**User**_

Audit this script.

---

_**Assistant**_

### [copilot_chat_capture.py](file:///d%3A/ActiveProjects/CommandCore-Hub_v2025.01.01/memory/full_chat/full_chat_scripts/copilot_chat_capture.py)

Audit findings and improvements:

1. **Error Handling in OCR Extraction**:
   - The `extract_chat_ocr` and `extract_chat_ocr_parallel` functions lack robust error handling for Vision API calls. Add exception handling to prevent crashes during OCR processing.

2. **Dynamic Region Capture**:
   - The `capture_region` function uses a global variable `region_box`. Ensure this variable is properly initialized before use to avoid runtime errors.

3. **Logging Enhancements**:
   - Improve logging messages for better traceability, especially in critical functions like `scroll_up`, `capture_region`, and OCR-related methods.

4. **Unused Imports**:
   - Remove unused imports like `subprocess` in certain parts of the script.

5. **Code Duplication**:
   - Refactor repetitive code blocks in `extract_chat_ocr` and `extract_chat_ocr_parallel` for better maintainability.

6. **Sanitization and Speaker Labeling**:
   - The `parse_chat_content` function has overly aggressive sanitization rules that may remove relevant content. Adjust regex patterns to preserve meaningful data.

7. **Global Variables**:
   - Replace global variables (`clicked_position`, `region_box`) with function arguments or encapsulate them in a class for better modularity.

8. **Hardcoded Paths**:
   - Replace hardcoded paths with configurable settings or environment variables for better portability.

Here are the suggested changes:

````python
# ...existing code...

def extract_chat_ocr(click_x, click_y):
    """
    Extract text from the captured region using OCR.

    Args:
        click_x (int): X-coordinate of the click.
        click_y (int): Y-coordinate of the click.

    Returns:
        str: Extracted text in natural language.
    """
    collected_text = []
    for i in range(scrolls):
        logging.info(f"Processing frame {i+1}/{scrolls}")
        try:
            img = capture_region()  # Updated to use capture_region
            img = preprocess_image(img)  # Preprocess the image before OCR

            # Vision API call with error handling
            response = client.text_detection(image=vision.Image(content=img.tobytes()))
            texts = response.text_annotations
            if texts:
                text = texts[0].description.strip()
                if text and text not in collected_text:
                    collected_text.append(text)
            scroll_up()
        except Exception as e:
            logging.error(f"Error during OCR extraction: {e}")
            break
    logging.info("OCR extraction completed.")
    return "\n\n".join(reversed(collected_text))

def extract_chat_ocr_parallel(click_x, click_y):
    """
    Extract text from the captured region using Google Vision API in parallel.

    Args:
        click_x (int): X-coordinate of the click.
        click_y (int): Y-coordinate of the click.

    Returns:
        str: Extracted text in natural language.
    """
    def process_frame(frame_number):
        try:
            logging.info(f"Processing frame {frame_number}")
            img = capture_region()  # Updated to use capture_region
            img_byte_arr = BytesIO()
            img.save(img_byte_arr, format='PNG')
            img_content = img_byte_arr.getvalue()

            # Vision API call
            image = vision.Image(content=img_content)
            response = client.text_detection(image=image)
            texts = response.text_annotations
            if texts:
                return texts[0].description.strip()
            return ""
        except Exception as e:
            logging.error(f"Error processing frame {frame_number}: {e}")
            return ""

    collected_text = []
    previous_text = None
    frame_number = 0

    logging.info("Starting OCR extraction with dynamic scrolling.")

    while True:
        frame_number += 1
        logging.info(f"Scrolling frame {frame_number}")
        scroll_up()

        text = process_frame(frame_number)
        if text and text != previous_text:
            collected_text.append(text)
            previous_text = text
        else:
            logging.info("No new text detected, stopping scrolling.")
            break

    logging.info("OCR extraction completed.")
    return "\n\n".join(reversed(collected_text))

def sanitize_filename(filename):
    """
    Sanitize the filename by removing invalid characters.

    Args:
        filename (str): Original filename.

    Returns:
        str: Sanitized filename.
    """
    sanitized = re.sub(r'[\\/:*?"<>|]', '_', filename)
    return sanitized[:100]  # Limit filename length to 100 characters

def parse_chat_content(raw_text):
    """
    Parse raw text to remove irrelevant metadata and label speakers.

    Args:
        raw_text (str): Raw text extracted from OCR.

    Returns:
        str: Parsed and labeled text.
    """
    cleaned = re.sub(r"^â†’ .+$", "", raw_text, flags=re.MULTILINE)
    cleaned = re.sub(r"^CommandCore-Hub_v\d{4}\.\d{2}\.\d{2}.*$", "", cleaned, flags=re.MULTILINE)

    # Remove lines that are all uppercase or just symbols
    cleaned = "\n".join(
        line for line in cleaned.splitlines()
        if not re.fullmatch(r"[A-Z\s]+", line.strip()) and not re.fullmatch(r"[-=~*#]+", line.strip())
    )

    # Normalize space, strip leading clutter
    cleaned = re.sub(r"\s{2,}", " ", cleaned)
    cleaned = re.sub(r"\bGPT-\d+\b", "", cleaned)
    cleaned = re.sub(r"^\s*GitHub Copilot\s*", "", cleaned, flags=re.MULTILINE)
    cleaned = re.sub(r"^\s*âœ“ Read .+", "", cleaned, flags=re.MULTILINE)
    cleaned = re.sub(r"^\s*Add Context.+", "", cleaned, flags=re.MULTILINE)
    cleaned = re.sub(r"^\s*4 files changed.+", "", cleaned, flags=re.MULTILINE)

    # Remove Copilot-specific phrases
    cleaned = re.sub(r"\b(Copilot|GPT|Command Palette|Ctrl\+Shift\+P|Alt\+Tab|file tree|sidebar)\b.*", "", cleaned, flags=re.IGNORECASE)

    # Label speakers
    labeled_blocks = []
    for block in cleaned.split("\n\n"):
        if "GitHub Copilot" in block or "Copilot" in block:
            labeled_blocks.append(f"**Copilot said:**\n{block.strip()}")
        elif any(keyword in block for keyword in ["CommandCore", "copilot_chat_capture.py"]):
            labeled_blocks.append(f"**Integrity Land Development said:**\n{block.strip()}")
        else:
            labeled_blocks.append(block.strip())

    return "\n\n".join(labeled_blocks).strip()

# ...existing code...
````

---

_**User**_



 Revert before this last change where you made the 8 changes.

---

_**Assistant**_

### [copilot_chat_capture.py](file:///d%3A/ActiveProjects/CommandCore-Hub_v2025.01.01/memory/full_chat/full_chat_scripts/copilot_chat_capture.py)

Reverting to the state before the last set of changes where 8 improvements were suggested.

````python
#!/usr/bin/env python3

import pyautogui
import mss
import os
import time
from datetime import datetime
import subprocess
from pynput import mouse
from io import BytesIO
from PIL import Image, ImageEnhance, ImageFilter
import logging
import re
import concurrent.futures
from google.cloud import vision

# === CONFIG ===
output_dir = r"D:/ActiveProjects/CommandCore-Hub_v2025.01.01/memory/full_chat/full_chat_logs"
summary_script = r"D:/ActiveProjects/CommandCore-Hub_v2025.01.01/memory/logs/summary/summary_scripts/summary.py"
tags = ["copilot_multimonitor_scroll"]
scrolls = 500
scroll_pause = 0.2
capture_width = 500

clicked_position = None
region_box = None

# Set up Google Vision API client
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "d:/ActiveProjects/CommandCore-Hub_v2025.01.01/memory/full_chat/full_chat_scripts/google_service_account_key.json"
client = vision.ImageAnnotatorClient()

# === LOGGING SETUP ===
logging.basicConfig(
    filename=os.path.join(output_dir, "copilot_chat_capture.log"),
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

def on_click(x, y, button, pressed):
    global clicked_position
    if pressed:
        clicked_position = (x, y)
        logging.info(f"Click detected at position: {clicked_position}")
        return False

def get_mouse_click():
    """
    Wait for a mouse click and return the position.

    Returns:
        tuple: (x, y) coordinates of the click.
    """
    with mouse.Listener(on_click=on_click) as listener:
        listener.join()
    return clicked_position

def find_monitor_containing(x, y):
    """
    Find the monitor containing the given coordinates.

    Args:
        x (int): X-coordinate of the click.
        y (int): Y-coordinate of the click.

    Returns:
        dict: Monitor details including dimensions.
    """
    with mss.mss() as sct:
        for monitor in sct.monitors:
            left, top, width, height = monitor["left"], monitor["top"], monitor["width"], monitor["height"]

            if left <= x < left + width and top <= y < top + height:
                logging.info(f"Monitor found for position ({x}, {y}): {monitor}")
                return monitor

        logging.warning("No specific monitor found, defaulting to primary monitor.")
        return sct.monitors[0]

def wait_for_bounding_box():
    """
    Prompt the user to click four times to define a bounding box.

    The clicks correspond to the top, left, right, and bottom boundaries.
    Stores the resulting box in a global variable `region_box`.
    """
    print("ðŸ–± Click TOP boundary...")
    top_click = get_mouse_click()
    print("ðŸ–± Click LEFT boundary...")
    left_click = get_mouse_click()
    print("ðŸ–± Click RIGHT boundary...")
    right_click = get_mouse_click()
    print("ðŸ–± Click BOTTOM boundary...")
    bottom_click = get_mouse_click()

    global region_box
    region_box = {
        'top': top_click[1],
        'left': left_click[0],
        'width': right_click[0] - left_click[0],
        'height': bottom_click[1] - top_click[1]
    }

    logging.info(f"Capture region defined: {region_box}")

def find_window_containing(x, y):
    """
    Find the window containing the given coordinates within the monitor.

    Args:
        x (int): X-coordinate of the click.
        y (int): Y-coordinate of the click.

    Returns:
        dict: Window details including dimensions.
    """
    with mss.mss() as sct:
        monitor = find_monitor_containing(x, y)
        left, top, width, height = monitor["left"], monitor["top"], monitor["width"], monitor["height"]

        # Adjust the region to half the monitor size for split-screen scenarios
        window_width = width // 2
        window_height = height

        if x < left + window_width:
            # Left half of the screen
            window_left = left
        else:
            # Right half of the screen
            window_left = left + window_width

        window_top = top

        logging.info(f"Window detected: left={window_left}, top={window_top}, width={window_width}, height={window_height}")

        # Debug log for window detection
        logging.info(f"Window dimensions calculated: left={window_left}, top={window_top}, width={window_width}, height={window_height}")

        return {
            "left": window_left,
            "top": window_top,
            "width": window_width,
            "height": window_height
        }

def capture_region():
    """
    Capture the region of the screen based on the global `region_box`.

    Returns:
        PIL.Image.Image: Captured image of the defined region.
    """
    with mss.mss() as sct:
        img = sct.grab(region_box)
        return Image.open(BytesIO(mss.tools.to_png(img.rgb, img.size)))

def capture_window_region(click_x, click_y):
    """
    Capture the region of the screen based on the window containing the click.

    Args:
        click_x (int): X-coordinate of the click.
        click_y (int): Y-coordinate of the click.

    Returns:
        PIL.Image.Image: Captured image of the window.
    """
    with mss.mss() as sct:
        window = find_window_containing(click_x, click_y)
        # Adjust the window region to crop off the bottom 60 pixels and the top 100 pixels
        window['height'] -= 60
        window['top'] += 100
        img = sct.grab(window)
        logging.info(f"Captured region from window: {window}")
        return Image.open(BytesIO(mss.tools.to_png(img.rgb, img.size)))

def scroll_up():
    try:
        logging.info("Attempting to scroll up...")
        pyautogui.scroll(3000)
        time.sleep(scroll_pause)
        logging.info("Scroll action completed.")

        # Placeholder for verifying scrolling success
        # Example: Check if the top of the chat window has changed
        # logging.info("Verifying scrolling success...")
        # if not content_changed():
        #     logging.warning("Scrolling did not change content.")
        #     raise RuntimeError("Scrolling failed.")

    except Exception as e:
        logging.error(f"Error during scrolling: {e}")
        raise

def preprocess_image(img):
    """
    Preprocess the image to improve OCR accuracy.

    Args:
        img (PIL.Image.Image): Captured image.

    Returns:
        PIL.Image.Image: Preprocessed image.
    """
    # Ensure the image is in grayscale mode
    if img.mode != "L":
        img = img.convert("L")

    # Apply binarization
    img = img.point(lambda x: 0 if x < 128 else 255, "1")

    # Convert back to grayscale for contrast enhancement
    img = img.convert("L")

    # Enhance contrast
    enhancer = ImageEnhance.Contrast(img)
    img = enhancer.enhance(2.0)

    # Apply a slight blur to reduce noise
    img = img.filter(ImageFilter.MedianFilter(size=3))

    return img

def extract_chat_ocr(click_x, click_y):
    """
    Extract text from the captured region using OCR.

    Args:
        click_x (int): X-coordinate of the click.
        click_y (int): Y-coordinate of the click.

    Returns:
        str: Extracted text in natural language.
    """
    collected_text = []
    for i in range(scrolls):
        logging.info(f"Processing frame {i+1}/{scrolls}")
        img = capture_region()  # Updated to use capture_region

        # Preprocess the image before OCR
        img = preprocess_image(img)

        # Replace pytesseract.image_to_string with Vision API calls
        text = client.text_detection(image=vision.Image(content=img.tobytes())).text_annotations[0].description.strip()

        if text and text not in collected_text:
            collected_text.append(text)
        scroll_up()
    logging.info("OCR extraction completed.")
    return "\n\n".join(reversed(collected_text))

def extract_chat_ocr_parallel(click_x, click_y):
    """
    Extract text from the captured region using Google Vision API in parallel.

    Args:
        click_x (int): X-coordinate of the click.
        click_y (int): Y-coordinate of the click.

    Returns:
        str: Extracted text in natural language.
    """
    def process_frame(frame_number):
        logging.info(f"Processing frame {frame_number}")
        img = capture_region()  # Updated to use capture_region

        # Convert the image to bytes for Vision API
        img_byte_arr = BytesIO()
        img.save(img_byte_arr, format='PNG')
        img_content = img_byte_arr.getvalue()

        # Create Vision API request
        image = vision.Image(content=img_content)
        response = client.text_detection(image=image)
        texts = response.text_annotations

        # Extract the detected text
        if texts:
            return texts[0].description.strip()
        return ""

    collected_text = []
    previous_text = None
    frame_number = 0

    logging.info("Starting OCR extraction with dynamic scrolling.")

    while True:
        frame_number += 1
        logging.info(f"Scrolling frame {frame_number}")
        scroll_up()

        try:
            text = process_frame(frame_number)
            if text and text != previous_text:
                collected_text.append(text)
                previous_text = text
            else:
                logging.info("No new text detected, stopping scrolling.")
                break
        except Exception as e:
            logging.error(f"Error processing frame {frame_number}: {e}")
            break

    logging.info("OCR extraction completed.")
    return "\n\n".join(reversed(collected_text))

def generate_title_from_text(text):
    """
    Generate a summary title for the markdown file based on the content.

    Prioritizes lines that contain actual action or speech.

    Args:
        text (str): Extracted text to analyze.

    Returns:
        str: Generated title summarizing the content.
    """
    # Prioritize lines that contain actual action or speech
    lines = [line.strip() for line in text.splitlines() if len(line.strip()) > 10 and not line.strip().isdigit()]
    important = [line for line in lines if "said:" in line or "click" in line.lower()]

    if important:
        title = important[0]
    elif lines:
        title = lines[0]
    else:
        title = "Captured Chat Log"

    return sanitize_filename(title[:100])

def sanitize_filename(filename):
    """
    Sanitize the filename by removing invalid characters.

    Args:
        filename (str): Original filename.

    Returns:
        str: Sanitized filename.
    """
    # Replace invalid characters with underscores
    sanitized = re.sub(r'[\\/:*?"<>|]', '_', filename)
    return sanitized[:100]  # Limit filename length to 100 characters

def save_markdown(text):
    """
    Save the extracted text into a markdown file with a generated title.

    Args:
        text (str): Extracted text to save.

    Returns:
        str: Path to the saved markdown file.
    """
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

    # Generate a descriptive title based on the content
    title = generate_title_from_text(text)
    if not title:
        title = "Captured Chat Log"

    # Sanitize the title for use as a filename
    sanitized_title = sanitize_filename(title)

    filename = f"{sanitized_title}_{timestamp}.md"
    output_path = os.path.join(output_dir, filename)

    os.makedirs(output_dir, exist_ok=True)
    token_count = len(text.split())

    with open(output_path, "w", encoding="utf-8") as f:
        f.write("---\n")
        f.write(f"source: copilot-multi\n")
        f.write(f"timestamp: {timestamp.replace('_', ' ')}\n")
        f.write(f"window_title: {sanitized_title}\n")
        f.write("vector_ready: true\n")
        f.write(f"tags: {tags}\n")
        f.write("---\n\n")
        f.write(f"# {sanitized_title}\n\n")
        f.write(text + "\n\n")
        f.write(f"---\nEstimated Token Count: {token_count}\n")

    logging.info(f"Log saved to: {output_path}")
    return output_path

def sync_to_vector_database(markdown_path):
    logging.info(f"Syncing markdown file to vector database: {markdown_path}")
    # Placeholder for vector database sync logic
    pass

def run_summary():
    logging.info("Running summary script.")
    subprocess.run(["python", summary_script])

def parse_chat_content(raw_text):
    """
    Parse raw text to remove irrelevant metadata and label speakers.

    Args:
        raw_text (str): Raw text extracted from OCR.

    Returns:
        str: Parsed and labeled text.
    """
    cleaned = re.sub(r"^â†’ .+$", "", raw_text, flags=re.MULTILINE)
    cleaned = re.sub(r"^CommandCore-Hub_v\d{4}\.\d{2}\.\d{2}.*$", "", cleaned, flags=re.MULTILINE)

    # Remove lines that are all uppercase or just symbols
    cleaned = "\n".join(
        line for line in cleaned.splitlines()
        if not re.fullmatch(r"[A-Z\s]+", line.strip()) and not re.fullmatch(r"[-=~*#]+", line.strip())
    )

    # Normalize space, strip leading clutter
    cleaned = re.sub(r"\s{2,}", " ", cleaned)
    cleaned = re.sub(r"\bGPT-\d+\b", "", cleaned)
    cleaned = re.sub(r"^\s*GitHub Copilot\s*", "", cleaned, flags=re.MULTILINE)
    cleaned = re.sub(r"^\s*âœ“ Read .+", "", cleaned, flags=re.MULTILINE)
    cleaned = re.sub(r"^\s*Add Context.+", "", cleaned, flags=re.MULTILINE)
    cleaned = re.sub(r"^\s*4 files changed.+", "", cleaned, flags=re.MULTILINE)

    # Remove Copilot-specific phrases
    cleaned = re.sub(r"\b(Copilot|GPT|Command Palette|Ctrl\+Shift\+P|Alt\+Tab|file tree|sidebar)\b.*", "", cleaned, flags=re.IGNORECASE)

    # Label speakers
    labeled_blocks = []
    for block in cleaned.split("\n\n"):
        if "GitHub Copilot" in block or "Copilot" in block:
            labeled_blocks.append(f"**Copilot said:**\n{block.strip()}")
        elif any(keyword in block for keyword in ["CommandCore", "copilot_chat_capture.py"]):
            labeled_blocks.append(f"**Integrity Land Development said:**\n{block.strip()}")
        else:
            labeled_blocks.append(block.strip())

    # Join labeled blocks with double newlines
    return "\n\n".join(labeled_blocks).strip()

import subprocess

def main():
    # ...existing code for chat logging...
    print("âœ… Chat logging completed. Triggering summary script...")
    
    # Trigger the summary script
    summary_script_path = r"d:\ActiveProjects\CommandCore-Hub_v2025.01.01\memory\logs\summary\summary_scripts\summary.py"
    subprocess.run(["python", summary_script_path], check=True)

if __name__ == "__main__":
    main()
````

---

